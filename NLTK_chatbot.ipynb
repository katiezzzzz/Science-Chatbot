{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=JzPgeRJfNo4&ab_channel=Intellipaat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "import wikipedia\n",
    "import wikipediaapi\n",
    "import json\n",
    "import pickle\n",
    "import sklearn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_topics = ['science', 'chemisry', 'physics', 'biology', 'sciences', 'cosmology', 'medicine', 'geology']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_pages = []\n",
    "for topic in search_topics:\n",
    "    results = wikipedia.search(topic, results=500)\n",
    "    for result in results:\n",
    "        if result not in wiki_pages:\n",
    "            wiki_pages.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_wiki = wikipediaapi.Wikipedia(language='en', extract_format=wikipediaapi.ExtractFormat.WIKI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove_str = [\"\\n\\n\", \"\\n\", \"\\\"\", \"\\u200a\", \"\\u2013\", \"\\u00e4\", \"\\u2014\", \"\\u00e9\", \"\\u00f6\", \"\\u00e0\"]\n",
    "remove_str = [\"\\n\\n\", \"\\n\", \"\\\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data = {}\n",
    "for page in wiki_pages:\n",
    "    text = wiki_wiki.page(page).text\n",
    "    for str in remove_str:\n",
    "        text = text.replace(str, ' ')\n",
    "    wiki_data[page] = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wiki_data_full.json', 'w') as f:\n",
    "    json.dump(wiki_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filehandler = open(b\"wiki_data_full.pickle\",\"wb\")\n",
    "pickle.dump(wiki_data,filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"wiki_data.pickle\",'rb')\n",
    "wiki_data = pickle.load(file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tokenizer divides a text into a list of sentences by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences. It must be trained on a large collection of plaintext in the target language before it can be used.\n",
    "\n",
    "The NLTK data package includes a pre-trained Punkt tokenizer for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/katiez/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNet is just another NLTK corpus reader\n",
    "\n",
    "Synset: a set of synonyms that share a common meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/katiez/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/katiez/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenisation\n",
    "sentence_tokens = nltk.sent_tokenize(wiki_data[list(wiki_data)[0]])\n",
    "word_tokens = nltk.word_tokenize(wiki_data[list(wiki_data)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punc_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punc_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he', 'love', 'eating', 'leaf']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LemNormalize('he loves eating leaves!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "stemmer.stem('eating')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define greeting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "greet_inputs = (\"hello\", \"hi\", \"hey\", \"whassup\", \"how are you?\")\n",
    "greet_responses = (\"hello\", \"hi\", \"Hey\", \"Hi there!\", \"Hey there!\")\n",
    "def greet(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in greet_inputs:\n",
    "            return random.choice(greet_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "a = ['1', '2']\n",
    "b = ['3']\n",
    "a.extend(b)\n",
    "print(a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert a collection of raw documents to a matrix of TF-IDF features. As tf–idf is very often used for text features, there is also another class called TfidfVectorizer that combines all the options of CountVectorizer and TfidfTransformer in a single model.\n",
    "\n",
    "While the tf–idf normalization is often very useful, there might be cases where the binary occurrence markers might offer better features. This can be achieved by using the binary parameter of CountVectorizer. In particular, some estimators such as Bernoulli Naive Bayes explicitly model discrete boolean random variables. Also, very short texts are likely to have noisy tf–idf values while the binary occurrence info is more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_input):\n",
    "    robot_response = ''\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    print(TfidfVec)\n",
    "    tfidf = TfidfVec.fit_transform(sentence_tokens)\n",
    "    print(sentence_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    # -1 is user_input\n",
    "    idx = vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if (req_tfidf == 0):\n",
    "        robot_response += \"Sorry, I can't understand you.\"\n",
    "    else:\n",
    "        robot_response += sentence_tokens[idx]\n",
    "    return robot_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tokens = nltk.sent_tokenize(wiki_data[list(wiki_data)[2]])\n",
    "len(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katiez/conda/envs/ML/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(183, 1616)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "tfidf = TfidfVec.fit_transform(sentence_tokens)\n",
    "array_of_feature = tfidf.toarray()\n",
    "array_of_feature.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define chatflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, I'm science robot. You can ask me any scientific questions! Please say goodbye before leaving!\n",
      "User: who is the father of science?\n",
      "TfidfVectorizer(stop_words='english',\n",
      "                tokenizer=<function LemNormalize at 0x7fc3f6fa0550>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katiez/conda/envs/ML/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 219)\t0.1897389562744652\n",
      "  (0, 6)\t0.21745256910072722\n",
      "  (0, 49)\t0.21745256910072722\n",
      "  (0, 1046)\t0.21745256910072722\n",
      "  (0, 564)\t0.21745256910072722\n",
      "  (0, 134)\t0.1588250525543729\n",
      "  (0, 341)\t0.21745256910072722\n",
      "  (0, 1077)\t0.15310355286859859\n",
      "  (0, 1262)\t0.21745256910072722\n",
      "  (0, 813)\t0.2336639933635954\n",
      "  (0, 1375)\t0.2059503805373333\n",
      "  (0, 1754)\t0.21745256910072722\n",
      "  (0, 552)\t0.1970285899577288\n",
      "  (0, 1687)\t0.2336639933635954\n",
      "  (0, 1266)\t0.1897389562744652\n",
      "  (0, 641)\t0.1897389562744652\n",
      "  (0, 1622)\t0.2336639933635954\n",
      "  (0, 690)\t0.18357565881405188\n",
      "  (0, 932)\t0.1326795737256002\n",
      "  (0, 1166)\t0.2336639933635954\n",
      "  (0, 257)\t0.21745256910072722\n",
      "  (0, 589)\t0.2336639933635954\n",
      "  (0, 1599)\t0.17823676771107125\n",
      "  (0, 1461)\t0.16656518571797915\n",
      "  (1, 286)\t0.22887110233768793\n",
      "  :\t:\n",
      "  (250, 746)\t0.23256132423543932\n",
      "  (250, 915)\t0.23256132423543932\n",
      "  (250, 623)\t0.19119224345132585\n",
      "  (250, 908)\t0.24554971589040744\n",
      "  (250, 780)\t0.4145911087884596\n",
      "  (250, 1461)\t0.09404357511025137\n",
      "  (251, 1739)\t0.23125807329696874\n",
      "  (251, 649)\t0.23125807329696874\n",
      "  (251, 1379)\t0.23125807329696874\n",
      "  (251, 1125)\t0.23125807329696874\n",
      "  (251, 1173)\t0.23125807329696874\n",
      "  (251, 1144)\t0.23125807329696874\n",
      "  (251, 980)\t0.46251614659393747\n",
      "  (251, 846)\t0.23125807329696874\n",
      "  (251, 344)\t0.2152135698779207\n",
      "  (251, 146)\t0.2038298135379791\n",
      "  (251, 978)\t0.2152135698779207\n",
      "  (251, 1039)\t0.2152135698779207\n",
      "  (251, 1389)\t0.2038298135379791\n",
      "  (251, 1759)\t0.2038298135379791\n",
      "  (251, 251)\t0.17640155377898947\n",
      "  (251, 1464)\t0.09114013751130569\n",
      "  (251, 1461)\t0.3297002963357977\n",
      "  (252, 666)\t0.9338526128126526\n",
      "  (252, 1461)\t0.3576580735045438\n",
      "Bot: the greek doctor hippocrates established the tradition of systematic medical science and is known as  the father of medicine .a turning point in the history of early philosophical science was socrates' example of applying philosophy to the study of human matters, including human nature, the nature of political communities, and human knowledge itself.\n",
      "User: hello\n",
      "Bot: Hey\n",
      "User: bye\n",
      "Bot: Bye!\n"
     ]
    }
   ],
   "source": [
    "flag = True\n",
    "print(\"Hi, I'm science robot. You can ask me any scientific questions! Please say goodbye before leaving!\")\n",
    "while (flag == True):\n",
    "    user_input = input()\n",
    "    print(f\"User: {user_input}\")\n",
    "    user_input = user_input.lower()\n",
    "    if (user_input != 'goodbye' and user_input != 'bye'):\n",
    "        if(user_input == 'thanks' or user_input == 'thank you'):\n",
    "            flag = False\n",
    "            print(\"Bot: You are welcome!\")\n",
    "        else:\n",
    "            if(greet(user_input) != None):\n",
    "                print(f\"Bot: {greet(user_input)}\")\n",
    "            else:\n",
    "                sentence_tokens.append(user_input)\n",
    "                word_tokens += nltk.word_tokenize(user_input)\n",
    "                final_words = list(set(word_tokens))\n",
    "                print(f\"Bot: {response(user_input)}\")\n",
    "                sentence_tokens.remove(user_input)\n",
    "    else:\n",
    "        flag = False\n",
    "        print(\"Bot: Bye!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b87ff53666c5e87906ecdd7442721966b002930c206cfecc9fbd67dd6037ee6c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
